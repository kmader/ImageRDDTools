{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import BasicProfiler\n",
    "# sc = pyspark.SparkContext(pyspark.SparkConf().setMaster(\"local[12]\"))\n",
    "import numpy as np\n",
    "import time\n",
    "# spark tools for making operations readable\n",
    "class NamedLambda(object):\n",
    "    \"\"\"\n",
    "    allows the use of named anonymous functions for arbitrary calculations\n",
    "    \"\"\"\n",
    "    def __init__(self, code_name, code_block, **add_args):\n",
    "        self.code_name = code_name\n",
    "        self.code_block = code_block\n",
    "        self.add_args = add_args\n",
    "        self.__name__ = code_name\n",
    "    def __call__(self, *cur_objs, **kwargs):\n",
    "        return self.code_block(*cur_objs, **kwargs, **self.add_args)\n",
    "    def __repr__(self):\n",
    "        return self.code_name\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "class FieldSelector(object):\n",
    "    \"\"\"\n",
    "    allows the use of named anonymous functions for selecting fields (makes dag's more readable)\n",
    "    \"\"\"\n",
    "    def __init__(self, field_name):\n",
    "        self.field_name = field_name\n",
    "        self.__name__ = \"Select Field: {}\".format(self.field_name)\n",
    "    def __call__(self, cur_obj):\n",
    "        try:\n",
    "            return cur_obj[self.field_name]\n",
    "        except:\n",
    "            return cur_obj._asdict()[self.field_name]\n",
    "    def __repr__(self):\n",
    "        return __name__\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "url_id = namedtuple('url', ['url'])\n",
    "src_tile_id = namedtuple('source_tile_position', ['img_id', 'z', 'x', 'y', 'type'])\n",
    "dest_tile_id = namedtuple('target_tile_position', ['img_id', 'z', 'x', 'y', 'type'])\n",
    "tile_item = namedtuple('tile_data', ['request' ,'target_tile_id', 'source_tile_id', 'source_tile_data', 'out_tile_data'])\n",
    "\n",
    "IM_W, IM_H = 60000, 60000\n",
    "IM_W, IM_H = 600, 600\n",
    "TILE_W, TILE_H = 2000, 2000\n",
    "TILE_W, TILE_H = 20, 20\n",
    "\n",
    "TILE_W, TILE_H = 60, 60\n",
    "#TILE_W, TILE_H = 200, 200\n",
    "TILES_PER_IMAGE = int(IM_W / TILE_W * IM_H / TILE_H)\n",
    "\n",
    "DELAY_SCALE = 0\n",
    "\n",
    "def stack(arr_list, axis = 0): \n",
    "    \"\"\"\n",
    "    since numpy 1.8.2 does not have the stack command\n",
    "    \"\"\"\n",
    "    assert axis == 0, \"Only works for axis 0\"\n",
    "    return np.vstack(map(lambda x: np.expand_dims(x,0), arr_list))\n",
    "\n",
    "def get_output_tiles(req_str):\n",
    "    return [(req_str,src_tile_id(req_str, -1, x, y,\"out\")) for x in np.arange(0, IM_W, TILE_W)\n",
    "            for y in np.arange(0, IM_H, TILE_H)]\n",
    "\n",
    "def calc_input_tiles(o_tile, n_size = 1, delay = 0):\n",
    "    \"\"\"\n",
    "    A simple 3x3 neighborhood with a delay second delay\n",
    "    \"\"\"\n",
    "    time.sleep(DELAY_SCALE*delay)\n",
    "    return [(o_tile, dest_tile_id(\n",
    "                img_id = o_tile.img_id,\n",
    "                z = o_tile.z,\n",
    "                type = \"in\",\n",
    "                x=ix, \n",
    "                y=iy)) \n",
    "            for ix in np.arange(o_tile.x-n_size*TILE_W,o_tile.x+(n_size+1)*TILE_W, TILE_W) if ix>0\n",
    "            for iy in np.arange(o_tile.y-n_size*TILE_H,o_tile.y+(n_size+1)*TILE_H, TILE_H) if iy>0]\n",
    "\n",
    "def pull_input_tile(i_tile, delay = 2.5):\n",
    "    \"\"\"\n",
    "    Generates a random, but unique 8 bit input tile given the source tile id i_tile\n",
    "    \"\"\"\n",
    "    assert i_tile.type.find(\"in\")==0\n",
    "    time.sleep(DELAY_SCALE*delay)\n",
    "    np.random.seed(i_tile.__hash__() % 4294967295) # should make process deterministic\n",
    "    return np.random.uniform(-127, 127, size = (TILE_W, TILE_H)).astype(np.int8)\n",
    "\n",
    "RECON_TIME = 1.0\n",
    "def partial_reconstruction(src_tile, targ_tile, tile_data, delay = RECON_TIME):\n",
    "    time.sleep(DELAY_SCALE*delay)\n",
    "    out_data = tile_data.copy()\n",
    "    out_data[out_data>20] = 0\n",
    "    return out_data\n",
    "\n",
    "def combine_reconstructions(many_slices):\n",
    "    \"\"\"\n",
    "    Bring a number of partial reconstructions together\n",
    "    \"\"\"\n",
    "    return np.sum(stack(many_slices,0),0)\n",
    "\n",
    "def full_reconstruction(src_tiles, pr_delay = RECON_TIME/2):\n",
    "    \"\"\"\n",
    "    Run the full reconstruction as one step\n",
    "    \"\"\"\n",
    "    out_images = [partial_reconstruction(src.source_tile_id, src.target_tile_id, src.source_tile_data, delay = pr_delay) for src in src_tiles]\n",
    "    return combine_reconstructions(out_images)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intermediate spark functions\n",
    "def ti_full_reconstruction(x):\n",
    "    k, src_tiles = x\n",
    "    return k, full_reconstruction(src_tiles)    \n",
    "\n",
    "def grp_tile_read(x):\n",
    "        src_tile_id, tile_items = x\n",
    "        tile_data = pull_input_tile(src_tile_id)\n",
    "        return [i._replace(source_tile_data = tile_data) for i in tile_items]\n",
    "\n",
    "def ti_partial_reconstruction(in_tile_item):\n",
    "    return in_tile_item._replace(\n",
    "        out_tile_data = partial_reconstruction(in_tile_item.source_tile_id,\n",
    "                                               in_tile_item.target_tile_id,\n",
    "                                               in_tile_item.source_tile_data),\n",
    "        source_tile_data = None # throw out the old data\n",
    "        )\n",
    "\n",
    "def ti_partial_collect(x):\n",
    "    k, in_tile_items = x # don't need the key\n",
    "    assert len(in_tile_items)>0, \"Cannot provide empty partial collecton set\"\n",
    "    iti_list = list(in_tile_items)\n",
    "    all_part_recon = map(lambda x: x.out_tile_data, in_tile_items)\n",
    "    \n",
    "    return (k, combine_reconstructions(all_part_recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _build_input(in_sc, reqs):\n",
    "    req_rdd = in_sc.parallelize(reqs)\n",
    "    out_tile_rdd = req_rdd.flatMap(get_output_tiles).repartition(100)\n",
    "    shuffle_tile_fields = NamedLambda(\"Shuffle Tile Fields\", lambda x: tile_item(x[0], x[1][0], x[1][1], None, None))\n",
    "    all_tile_rdd = out_tile_rdd.flatMapValues(calc_input_tiles).map(shuffle_tile_fields)\n",
    "    return all_tile_rdd\n",
    "def build_naive_pipe(in_sc, reqs):\n",
    "    all_tile_rdd = _build_input(in_sc, reqs)\n",
    "    # parallel reading of the data\n",
    "    read_fcn = NamedLambda(\"pull_input_tile\", lambda i: i._replace(source_tile_data = pull_input_tile(i.source_tile_id)))\n",
    "    all_tile_rdd_data = all_tile_rdd.map(read_fcn)\n",
    "    # parallel combining of the tiles\n",
    "    recon_tiles_rdd = all_tile_rdd_data.groupBy(FieldSelector('target_tile_id')).map(ti_full_reconstruction)\n",
    "    return recon_tiles_rdd\n",
    "\n",
    "def build_grpread_pipe(in_sc, reqs):\n",
    "    all_tile_rdd = _build_input(in_sc, reqs)\n",
    "    single_read_tiles_rdd = all_tile_rdd.groupBy(FieldSelector('source_tile_id')).flatMap(grp_tile_read)\n",
    "    gr_recon_tiles_rdd = single_read_tiles_rdd.groupBy(FieldSelector('target_tile_id')).map(ti_full_reconstruction)\n",
    "    return gr_recon_tiles_rdd\n",
    "\n",
    "def build_partialrecon_pipe(in_sc, reqs):\n",
    "    all_tile_rdd = _build_input(in_sc, reqs)\n",
    "    # group together all files by source tile and then read that source tile and put in into every item\n",
    "    single_read_tiles_rdd = all_tile_rdd.groupBy(FieldSelector('source_tile_id')).flatMap(grp_tile_read)\n",
    "    # run a partial reconstruction on every item\n",
    "    partial_recon_tiles_rdd = single_read_tiles_rdd.map(ti_partial_reconstruction)\n",
    "    # combine all the partial reconstructions to create the final reconstruction\n",
    "    full_recon_tiles_rdd = partial_recon_tiles_rdd.groupBy(FieldSelector('target_tile_id')).map(ti_partial_collect)\n",
    "    return full_recon_tiles_rdd\n",
    "test_reqs = list(map(url_id, ['test1', 'test2', 'test3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_data(request='test2', target_tile_id=source_tile_position(img_id='test2', z=-1, x=0, y=0, type='out'), source_tile_id=target_tile_position(img_id='test2', z=-1, x=60, y=60, type='in'), source_tile_data=None, out_tile_data=None)\n"
     ]
    }
   ],
   "source": [
    "print(_build_input(sc, ['test1', 'test2']).first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple / Naive Approach\n",
    "The Simple Naive Approach does not attempt to group together operations reading the same input times and just runs each independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recon_tiles_rdd = build_naive_pipe(sc, ['test1', 'test2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Results 200\n",
      "CPU times: user 26.8 ms, sys: 9.58 ms, total: 36.4 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_shapes = recon_tiles_rdd.mapValues(lambda x: x.shape).collect()\n",
    "print('All Results', len(all_shapes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Read Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gr_recon_tiles_rdd = build_grpread_pipe(sc, ['test1', 'test2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Results 200\n",
      "CPU times: user 37 ms, sys: 13.5 ms, total: 50.4 ms\n",
      "Wall time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_shapes = gr_recon_tiles_rdd.mapValues(lambda x: x.shape).collect()\n",
    "print('All Results', len(all_shapes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach\n",
    "Partial Results to Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_recon_tiles_rdd = build_partialrecon_pipe(sc, ['test1', 'test2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Results 200\n",
      "CPU times: user 36.1 ms, sys: 13.1 ms, total: 49.1 ms\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_shapes = full_recon_tiles_rdd.mapValues(lambda x: x.shape).collect()\n",
    "print('All Results',len(all_shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check every element to ensure they are identical\n",
    "for i, (x,y) in enumerate(zip(full_recon_tiles_rdd.first()[1].flatten(), \n",
    "                              recon_tiles_rdd.first()[1].flatten())):\n",
    "    assert x == y, \"Index {} doesnt match ({} != {})\".format(i, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing Tiles and Output\n",
    "Here we compress the tiles and then write the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "def compress_tile(in_tile):\n",
    "    out_stream = BytesIO()\n",
    "    np.savez_compressed(out_stream, out_tile = in_tile)\n",
    "    out_stream.seek(0) # restart at beginning\n",
    "    return out_stream\n",
    "\n",
    "t_image = np.random.uniform(0, 255, size = (2000, 2000, 4)).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cpickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-c29422ca393b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cpickle'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.25 s, sys: 246 ms, total: 6.49 s\n",
      "Wall time: 6.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ios = compress_tile(t_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "def compress_tile(in_tile):\n",
    "    out_stream = BytesIO()\n",
    "    np.savez_compressed(out_stream, out_tile = in_tile)\n",
    "    out_stream.seek(0) # restart at beginning\n",
    "    return b''.join(out_stream.readlines())\n",
    "\n",
    "import tempfile\n",
    "def write_tiles(kvarg):\n",
    "    img_info, tile_list = kvarg\n",
    "    out_data = b''\n",
    "    out_file = tempfile.NamedTemporaryFile(suffix = 'jp2', prefix = str(img_info))\n",
    "    for tile_info, tile_data in tile_list:\n",
    "        out_file.write(tile_data)\n",
    "    \n",
    "    return (img_info,out_file.name)\n",
    "def build_full_pipe(in_sc, reqs, read_pipe = build_grpread_pipe):\n",
    "    full_recon_tiles_rdd = build_grpread_pipe(in_sc, reqs)\n",
    "    all_image_rdd = full_recon_tiles_rdd.mapValues(compress_tile).groupBy(NamedLambda(\"Select Field : img_id\",lambda x: x[0].img_id))\n",
    "    return all_image_rdd.map(write_tiles)\n",
    "\n",
    "def build_parallelio_pipe(in_sc, reqs, read_pipe = build_grpread_pipe):\n",
    "    full_recon_tiles_rdd = build_grpread_pipe(in_sc, reqs)\n",
    "    all_image_rdd = full_recon_tiles_rdd.mapValues(compress_tile).groupBy(NamedLambda(\"Select Field : img_id\",lambda x: x[0].img_id))\n",
    "    return all_image_rdd.saveAsPickleFile('test_output_pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_pipe = build_full_pipe(sc, test_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(url(url='test3'),\n",
       " \"/var/folders/87/qdzwp8795sj37r777qjqyybm0000gn/T/url(url='test3')jbkiejxijp2\")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipe.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 21 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_image_rdd.saveAsPickleFile('test_output_pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Keys and Heavy Values\n",
    "## DataFrame vs RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a cached performant RDD to start with\n",
    "x_to_tile = lambda x: tile_id(\"\",0,int(x),int(x),\"in\")\n",
    "keys_rdd = sc.parallelize(range(101),20).map(x_to_tile).cache()\n",
    "_ = keys_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Standard RDDs\n",
    "Using a standard RDD the entire data has to be loaded / processed in order to check the x value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kimg_rdd = keys_rdd.map(lambda x: (x, pull_input_tile(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 ms, sys: 12.5 ms, total: 30.1 ms\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tile_id(img_id='', z=0, x=100, y=100, type='in'),\n",
       "  array([[-111,   57,   87, ...,    3,   42,  -43],\n",
       "         [-105,    4,  -40, ...,   38,   74,  -34],\n",
       "         [  40,  -65,  -50, ...,   55,  -59, -113],\n",
       "         ..., \n",
       "         [ -25,   27,  123, ...,  -78,   83,   78],\n",
       "         [ -16,  -78,  -50, ..., -111,  -73,  123],\n",
       "         [ -82,  -30,  -46, ...,  102,   -4,  104]], dtype=int8))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "kimg_rdd.filter(lambda kv_data: kv_data[0].x>99).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DataFrames\n",
    "Using DataFrames the exact same query can be conducted without looking at the image column at all. Here the image column is only examined at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as sq_types\n",
    "kmeta_df = sqlContext.createDataFrame(keys_rdd.map(lambda x: x._asdict()))\n",
    "# applying python functions to DataFrames is more difficult and requires using typed UDFs\n",
    "twod_arr_type = sq_types.ArrayType(sq_types.ArrayType(sq_types.IntegerType()))\n",
    "# the pull_input_tile function is wrapped into a udf to it can be applied to create the new image column\n",
    "# numpy data is not directly supported and typed arrays must be used instead therefor we run the .tolist command\n",
    "pull_tile_udf = F.udf(lambda x: pull_input_tile(x_to_tile(x)).tolist(), returnType = twod_arr_type)\n",
    "kimg_df = kmeta_df.withColumn('Image', pull_tile_udf(kmeta_df['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+---+--------------------+\n",
      "|img_id|type|  x|  y|  z|               Image|\n",
      "+------+----+---+---+---+--------------------+\n",
      "|      |  in|100|100|  0|[WrappedArray(-11...|\n",
      "+------+----+---+---+---+--------------------+\n",
      "\n",
      "CPU times: user 4.45 ms, sys: 2.71 ms, total: 7.16 ms\n",
      "Wall time: 6.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s_query = kimg_df.where(kimg_df['x']>99)\n",
    "s_query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+---+--------------------+\n",
      "|img_id|type|  x|  y|  z|               Image|\n",
      "+------+----+---+---+---+--------------------+\n",
      "|      |  in| 27| 27|  0|[WrappedArray(110...|\n",
      "+------+----+---+---+---+--------------------+\n",
      "\n",
      "CPU times: user 4.01 ms, sys: 2.34 ms, total: 6.35 ms\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kimg_df.where(kimg_df['x']==27).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-111,   57,   87, ...,    3,   42,  -43],\n",
       "       [-105,    4,  -40, ...,   38,   74,  -34],\n",
       "       [  40,  -65,  -50, ...,   55,  -59, -113],\n",
       "       ..., \n",
       "       [ -25,   27,  123, ...,  -78,   83,   78],\n",
       "       [ -16,  -78,  -50, ..., -111,  -73,  123],\n",
       "       [ -82,  -30,  -46, ...,  102,   -4,  104]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the array to make sure it matches\n",
    "iv_arr = np.array(s_query.first().Image)\n",
    "iv_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test cartesian product later kmeta_df.join(s_query, on = [\"x\", \"y\"], how = 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [img_id#0, type#1, x#2L, y#3L, z#4L, pythonUDF0#39 AS Image#11]\n",
      "+- BatchEvalPython [<lambda>(x#2L)], [img_id#0, type#1, x#2L, y#3L, z#4L, pythonUDF0#39]\n",
      "   +- *Filter (isnotnull(x#2L) && (x#2L > 99))\n",
      "      +- Scan ExistingRDD[img_id#0,type#1,x#2L,y#3L,z#4L]\n"
     ]
    }
   ],
   "source": [
    "s_query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pt_2d_type = sq_types.StructType(fields = [sq_types.StructField(\"_1\", sq_types.IntegerType()), \n",
    "                     sq_types.StructField(\"_2\", sq_types.IntegerType())])\n",
    "brightest_point_udf = F.udf(lambda x: np.unravel_index(np.argmax(x), dims = np.shape(x)), returnType = pt_2d_type)\n",
    "mean_point_udf = F.udf(lambda x: float(np.mean(x)), returnType = sq_types.DoubleType())\n",
    "kimg_max_df = kimg_df.withColumn('MeanPoint', mean_point_udf(kimg_df['Image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+---+--------------------+-----------+\n",
      "|img_id|type|  x|  y|  z|               Image|  MeanPoint|\n",
      "+------+----+---+---+---+--------------------+-----------+\n",
      "|      |  in| 89| 89|  0|[WrappedArray(-12...|-0.01192325|\n",
      "|      |  in| 90| 90|  0|[WrappedArray(80,...| 0.03578475|\n",
      "|      |  in| 91| 91|  0|[WrappedArray(23,...|  -0.047191|\n",
      "+------+----+---+---+---+--------------------+-----------+\n",
      "\n",
      "CPU times: user 5.82 ms, sys: 3.92 ms, total: 9.74 ms\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "four_img_query = kimg_max_df.where(kimg_max_df['x']>88).where(kimg_max_df['y']<92)\n",
    "four_img_query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [img_id#0, type#1, x#2L, y#3L, z#4L, pythonUDF0#62 AS Image#11, pythonUDF1#63 AS MeanPoint#40]\n",
      "+- BatchEvalPython [<lambda>(x#2L), <lambda>(<lambda>(x#2L))], [img_id#0, type#1, x#2L, y#3L, z#4L, pythonUDF0#62, pythonUDF1#63]\n",
      "   +- *Filter (((isnotnull(x#2L) && (x#2L > 88)) && isnotnull(y#3L)) && (y#3L < 92))\n",
      "      +- Scan ExistingRDD[img_id#0,type#1,x#2L,y#3L,z#4L]\n"
     ]
    }
   ],
   "source": [
    "four_img_query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?all_tile_rdd.prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test2',\n",
       " (tile_id(img_id='test2', z=-1, x=0, y=0, type='out'),\n",
       "  tile_id(img_id='test2', z=-1, x=2000, y=2000, type='in')))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tile_rdd.prev.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tile_item(request='test2', target_tile_id=tile_id(img_id='test2', z=-1, x=0, y=0, type='out'), source_tile_id=tile_id(img_id='test2', z=-1, x=2000, y=2000, type='in'), source_tile_data=None, out_tile_data=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tile_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.rdd.PipelinedRDD.__init__.<locals>.pipeline_func>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tile_rdd.func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['            def pipeline_func(split, iterator):\\n',\n",
       "  '                return func(split, prev_func(split, iterator))\\n'],\n",
       " 2370)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(all_tile_rdd.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pydot\n",
    "def show_dag(cur_rdd, o_file):\n",
    "    from IPython.display import SVG\n",
    "    b_dot = trace_dag(cur_rdd)\n",
    "    b_dot.write_svg(o_file)\n",
    "    return SVG(o_file)\n",
    "def trace_dag(cur_rdd):\n",
    "    dot_graph = pydot.Dot(graph_type='digraph')\n",
    "    dot_graph.set('rankdir', 'TB')\n",
    "    dot_graph.set('concentrate', False)\n",
    "    dot_graph.set_node_defaults(shape='record')\n",
    "    add_rdd_node(dot_graph, cur_rdd)\n",
    "    return dot_graph\n",
    "\n",
    "def add_rdd_node(dot_graph, cur_rdd):\n",
    "    \"\"\"\n",
    "    Recursive function to add all RDDs to a DAG given a starting node\n",
    "    \"\"\"\n",
    "    def make_node(name, label):\n",
    "        cur_node = pydot.Node(name)\n",
    "        cur_node.set_label(label)\n",
    "        dot_graph.add_node(cur_node)\n",
    "        return cur_node\n",
    "    def make_link(a_node, b_node, label = None, width = 1, style='dashed'):\n",
    "        cur_edge = pydot.Edge(a_node,b_node)\n",
    "        cur_edge.set_penwidth(width)\n",
    "        cur_edge.set_style(style)\n",
    "        if label is not None: cur_edge.set_label(label)\n",
    "        dot_graph.add_edge(cur_edge)\n",
    "        return cur_edge\n",
    "    try:\n",
    "        op_name = cur_rdd.command_args[0]\n",
    "        cmd_name = cur_rdd.command_args[1]\n",
    "        func_obj = cur_rdd.command_args[1].get('apply_func',lambda x: x)\n",
    "        print(op_name, 'type',cur_rdd.type, func_obj, '->', func_name)\n",
    "    except:\n",
    "        print(\"must be a real rdd\")\n",
    "        op_name = \"MapPartitions\"\n",
    "        func_obj = cur_rdd.__dict__.get('func',lambda x: x)\n",
    "    \n",
    "    func_name = 'Custom' if func_obj.__name__.find('lambda')>=0 else func_obj.__name__\n",
    "    func_name = 'Custom' if func_name.find('pipeline_func')>=0 else func_name\n",
    "    if op_name == 'parallelize': func_name = 'parallelize : {}'.format(func_name)\n",
    "    \n",
    "    label = '%s\\n|{type:|count:}|{{%s}|{%s}}' % (func_name, LocalRDD.typeInfo(cur_rdd.first()), cur_rdd.count())\n",
    "    c_node = make_node(cur_rdd.id(), label)\n",
    "    if type(cur_rdd) is list:\n",
    "        parent_nodes = []\n",
    "    else:\n",
    "        parent_nodes = cur_rdd.__dict__.get('prev',[])\n",
    "        if type(parent_nodes) is not list: parent_nodes = [parent_nodes]\n",
    "    \n",
    "    for inode in parent_nodes:\n",
    "        next_node, next_op_name = add_rdd_node(dot_graph, inode)\n",
    "        l_count = 3 if op_name.find('flat')>=0 else 1\n",
    "        for l_id in range(l_count):\n",
    "            make_link(next_node, c_node, label = op_name if l_id is 0 else None)\n",
    "    return c_node, op_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must be a real rdd\n",
      "must be a real rdd\n",
      "must be a real rdd\n"
     ]
    }
   ],
   "source": [
    "cdag = trace_dag(all_tile_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<bound method RDD.id of MapPartitionsRDD[5] at repartition at NativeMethodAccessorImpl.java:-2>: [{'attributes': {'label': 'Custom\\n|{type:|count:}|{{(string, tile_id)}|{18}}'},\n",
       "   'name': <bound method RDD.id of MapPartitionsRDD[5] at repartition at NativeMethodAccessorImpl.java:-2>,\n",
       "   'parent_graph': <pydot.Dot at 0x10a2b1cf8>,\n",
       "   'parent_node_list': None,\n",
       "   'port': None,\n",
       "   'sequence': 4,\n",
       "   'type': 'node'}],\n",
       " 'node': [{'attributes': {'shape': 'record'},\n",
       "   'name': 'node',\n",
       "   'parent_graph': <pydot.Dot at 0x10a2b1cf8>,\n",
       "   'parent_node_list': None,\n",
       "   'port': None,\n",
       "   'sequence': 1,\n",
       "   'type': 'node'}],\n",
       " <bound method PipelinedRDD.id of PythonRDD[80] at RDD at PythonRDD.scala:48>: [{'attributes': {'label': 'Custom\\n|{type:|count:}|{{tile_item}|{72}}'},\n",
       "   'name': <bound method PipelinedRDD.id of PythonRDD[80] at RDD at PythonRDD.scala:48>,\n",
       "   'parent_graph': <pydot.Dot at 0x10a2b1cf8>,\n",
       "   'parent_node_list': None,\n",
       "   'port': None,\n",
       "   'sequence': 2,\n",
       "   'type': 'node'}],\n",
       " <bound method PipelinedRDD.id of PythonRDD[11] at RDD at PythonRDD.scala:48>: [{'attributes': {'label': 'func\\n|{type:|count:}|{{(string, (tile_id, tile_id))}|{72}}'},\n",
       "   'name': <bound method PipelinedRDD.id of PythonRDD[11] at RDD at PythonRDD.scala:48>,\n",
       "   'parent_graph': <pydot.Dot at 0x10a2b1cf8>,\n",
       "   'parent_node_list': None,\n",
       "   'port': None,\n",
       "   'sequence': 3,\n",
       "   'type': 'node'}]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdag.obj_dict['nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must be a real rdd\n",
      "must be a real rdd\n",
      "must be a real rdd\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"243pt\" viewBox=\"0.00 0.00 251.95 243.00\" width=\"252pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 239)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-239 247.952,-239 247.952,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 80 -->\n",
       "<g class=\"node\" id=\"node1\"><title>80</title>\n",
       "<polygon fill=\"none\" points=\"33.814,-0.5 33.814,-44.5 210.138,-44.5 210.138,-0.5 33.814,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"63.5967\" y=\"-18.3\">Custom</text>\n",
       "<polyline fill=\"none\" points=\"93.3794,-0.5 93.3794,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.876\" y=\"-29.3\">type:</text>\n",
       "<polyline fill=\"none\" points=\"93.3794,-22.5 144.373,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.876\" y=\"-7.3\">count:</text>\n",
       "<polyline fill=\"none\" points=\"144.373,-0.5 144.373,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.255\" y=\"-29.3\">tile_item</text>\n",
       "<polyline fill=\"none\" points=\"144.373,-22.5 210.138,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.873\" y=\"-7.3\">72</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g class=\"node\" id=\"node2\"><title>11</title>\n",
       "<polygon fill=\"none\" points=\"0,-95.5 0,-139.5 243.952,-139.5 243.952,-95.5 0,-95.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"20.438\" y=\"-113.3\">func</text>\n",
       "<polyline fill=\"none\" points=\"40.876,-95.5 40.876,-139.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.3726\" y=\"-124.3\">type:</text>\n",
       "<polyline fill=\"none\" points=\"40.876,-117.5 91.8691,-117.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.3726\" y=\"-102.3\">count:</text>\n",
       "<polyline fill=\"none\" points=\"91.8691,-95.5 91.8691,-139.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.911\" y=\"-124.3\">(string, (tile_id, tile_id))</text>\n",
       "<polyline fill=\"none\" points=\"91.8691,-117.5 243.952,-117.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.869\" y=\"-102.3\">72</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;80 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>11-&gt;80</title>\n",
       "<path d=\"M121.976,-95.3969C121.976,-83.3873 121.976,-68.1175 121.976,-54.742\" fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\"/>\n",
       "<polygon fill=\"black\" points=\"125.476,-54.5196 121.976,-44.5196 118.476,-54.5197 125.476,-54.5196\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.642\" y=\"-65.8\">MapPartitions</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g class=\"node\" id=\"node3\"><title>5</title>\n",
       "<polygon fill=\"none\" points=\"16.7036,-190.5 16.7036,-234.5 227.249,-234.5 227.249,-190.5 16.7036,-190.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"46.4863\" y=\"-208.3\">Custom</text>\n",
       "<polyline fill=\"none\" points=\"76.269,-190.5 76.269,-234.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.766\" y=\"-219.3\">type:</text>\n",
       "<polyline fill=\"none\" points=\"76.269,-212.5 127.262,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.766\" y=\"-197.3\">count:</text>\n",
       "<polyline fill=\"none\" points=\"127.262,-190.5 127.262,-234.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.255\" y=\"-219.3\">(string, tile_id)</text>\n",
       "<polyline fill=\"none\" points=\"127.262,-212.5 227.249,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.762\" y=\"-197.3\">18</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>5-&gt;11</title>\n",
       "<path d=\"M121.976,-190.397C121.976,-178.387 121.976,-163.117 121.976,-149.742\" fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\"/>\n",
       "<polygon fill=\"black\" points=\"125.476,-149.52 121.976,-139.52 118.476,-149.52 125.476,-149.52\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"161.642\" y=\"-160.8\">MapPartitions</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "show_dag(all_tile_rdd, 'pyspark_rdd.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocalRDD(object):\n",
    "    def __init__(self, items, parent, command, code = '', **args):\n",
    "        self.items = list(items)\n",
    "        self.prev = parent\n",
    "        self.command_args = (command, args)\n",
    "        self.code = code\n",
    "    \n",
    "    def first(self):\n",
    "        return self.items[0]\n",
    "    \n",
    "    def collect(self):\n",
    "        return self.items\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def _transform(self, op_name, in_func, lapply_func, **args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_func is the function the user supplied\n",
    "            lapply_func is the function to actually apply\n",
    "        \"\"\"\n",
    "        try:\n",
    "            trans_func_code = inspect.getsourcelines(in_func)\n",
    "        except:\n",
    "            trans_func_code = ''\n",
    "        new_list = lapply_func(self.items)\n",
    "        return LocalRDD(new_list, [self], op_name, apply_func = in_func, code = trans_func_code, **args)\n",
    "        \n",
    "    def map(self, apply_func):\n",
    "        return self._transform('map', apply_func, \n",
    "                               lapply_func =  lambda x_list: [apply_func(x) for x in x_list])\n",
    "    \n",
    "    def mapValues(self, apply_func):\n",
    "        return self._transform('mapValues', apply_func, \n",
    "                               lapply_func = lambda x_list: [(k, apply_func(v)) for (k,v) in x_list])\n",
    "    \n",
    "    def flatMap(self, apply_func):\n",
    "        return self._transform('flatMap', apply_func, \n",
    "                               lapply_func = lambda x_list: cflatten([apply_func(x) for x in x_list]))\n",
    "    \n",
    "    def flatMapValues(self, apply_func):\n",
    "        return self._transform('flatMapValues', apply_func, \n",
    "                               lapply_func = lambda x_list: cflatten([[(k, y) for y in apply_func(v)] for (k,v) in x_list]))\n",
    "    \n",
    "    def groupBy(self, apply_func):\n",
    "        def gb_func(x_list):\n",
    "            o_dict = defaultdict(list)\n",
    "            for i in x_list:\n",
    "                o_dict[apply_func(i)]+= [i]\n",
    "            return list(o_dict.items())\n",
    "        return self._transform('groupBy', apply_func, \n",
    "                               lapply_func = gb_func)\n",
    "    \n",
    "    \n",
    "    def saveAsPickleFile(self, filename):\n",
    "        return self._transform('saveAsPickleFile', pickle.dump, \n",
    "                               lapply_func =  lambda x_list: [(x, filename) for x in x_list])\n",
    "    def repartition(self, *args):\n",
    "        return self\n",
    "                               \n",
    "    \n",
    "    # collection of informational functions\n",
    "    @staticmethod\n",
    "    def typeInfo(obj):\n",
    "        if type(obj) is tuple:\n",
    "            return '({})'.format(', '.join(map(LocalRDD.typeInfo,obj)))\n",
    "        elif type(obj) is list:\n",
    "            return 'list({})'.format(LocalRDD.typeInfo(obj[0]))\n",
    "        else:\n",
    "            ctype_name = type(obj).__name__\n",
    "            if ctype_name == 'ndarray': return '{}{}'.format(ctype_name,obj.shape)\n",
    "            elif ctype_name == 'str': return 'string'\n",
    "            else: return ctype_name\n",
    "    \n",
    "    @property\n",
    "    def type(self):\n",
    "        return LocalRDD.typeInfo(self.first())\n",
    "    \n",
    "    @property\n",
    "    def key_type(self):\n",
    "        return LocalRDD.typeInfo(self.items[0][0])\n",
    "    \n",
    "    @property\n",
    "    def value_type(self):\n",
    "        return LocalRDD.typeInfo(self.items[0][1])\n",
    "    \n",
    "    @property\n",
    "    def id(self):\n",
    "        return (str(self.items), tuple(self.parent), str(self.command_args)).__hash__()\n",
    "        \n",
    "\n",
    "class LocalSparkContext(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def parallelize(self, in_list):\n",
    "        return LocalRDD(in_list, [], 'parallelize', in_list = in_list)\n",
    "\n",
    "class NamedLambda(object):\n",
    "    \"\"\"\n",
    "    allows the use of named anonymous functions for arbitrary calculations\n",
    "    \"\"\"\n",
    "    def __init__(self, code_name, code_block, **add_args):\n",
    "        self.code_name = code_name\n",
    "        self.code_block = code_block\n",
    "        self.add_args = add_args\n",
    "    def __call__(self, cur_obj):\n",
    "        return self.code_block(cur_obj, **self.add_args)\n",
    "    def __repr__(self):\n",
    "        return self.code_name\n",
    "    \n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return self.__repr__()\n",
    "class FieldSelector(object):\n",
    "    \"\"\"\n",
    "    allows the use of named anonymous functions for selecting fields (makes dag's more readable)\n",
    "    \"\"\"\n",
    "    def __init__(self, field_name):\n",
    "        self.field_name = field_name\n",
    "    def __call__(self, cur_obj):\n",
    "        try:\n",
    "            return cur_obj[self.field_name]\n",
    "        except:\n",
    "            return cur_obj._asdict()[self.field_name]\n",
    "    def __repr__(self):\n",
    "        return \"Select Field: {}\".format(self.field_name)\n",
    "    \n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
